name: Scrape VesselFinder (4x daily)

on:
  schedule:
    - cron: "13 0,6,12,18 * * *"   # 00:13, 06:13, 12:13, 18:13 UTC
  workflow_dispatch: {}

permissions:
  contents: read

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r scripts/requirements.txt

      # AWS creds so the script can upload to S3
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ secrets.AWS_REGION }}

      # ðŸ‘‡ Pass DATABASE_URL + S3 info to your Python script
      - name: Run VesselFinder scraper (DB â†’ CSV â†’ S3)
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          S3_BUCKET:    ${{ secrets.S3_BUCKET }}
          S3_PREFIX:    ${{ secrets.S3_PREFIX }}   # e.g., "berbera"
          AWS_REGION:   ${{ secrets.AWS_REGION }}
        run: python scripts/vf_scrape.py

      # Optional: keep artifacts for debugging/download
      - name: Upload CSVs as workflow artifact
        uses: actions/upload-artifact@v4
        with:
          name: vf-snapshots
          path: |
            data/vf_snapshot.csv
            data/vf_snapshots/**
